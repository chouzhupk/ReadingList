# ReadingList
Paper and talk grass list.

[Amazing Reddit](https://www.reddit.com/r/MachineLearning/)

### 07252017

[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)

[Deep Networks with Stochastic Depth](https://arxiv.org/pdf/1603.09382.pdf)

[RESNET IN RESNET: GENERALIZING RESIDUAL ARCHITECTURES](https://arxiv.org/pdf/1603.08029.pdf)

### 07192017

[A fast and simple algorithm for training neural probabilistic language models](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf)

[EXTENSIONS OF RECURRENT NEURAL NETWORK LANGUAGE MODEL](file:///home/layer6s3/Downloads/Extensions%20of%20recurrent%20neural%20network%20language%20model.pdf)

### 07182017

[Efficient softmax approximation for GPUs](https://arxiv.org/pdf/1609.04309.pdf)

[Strategies for Training Large Vocabulary Neural Language Models](https://arxiv.org/pdf/1512.04906.pdf)

[Strategies for Training Large Scale Neural Network Language Models](https://www.microsoft.com/en-us/research/wp-content/uploads/2011/12/ASRU-2011.pdf)

### 07122017

[VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition](http://www.dimatura.net/publications/voxnet_maturana_scherer_iros15.pdf)

[WAVENET: A GENERATIVE MODEL FOR RAW AUDIO](https://arxiv.org/pdf/1609.03499.pdf)

[The surprising secret identity of the semidefinite relaxation of K-means: manifold learning](https://arxiv.org/pdf/1706.06028.pdf)

[An overview of gradient descent optimization algorithms∗](https://arxiv.org/pdf/1609.04747.pdf)

[Unsupervised Learning of Depth and Ego-Motion from Video](https://arxiv.org/pdf/1704.07813.pdf)

[Multi-view Supervision for Single-view Reconstruction via Differentiable Ray Consistency](https://arxiv.org/pdf/1704.06254.pdf)

### 07072017

[Momentum](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)

[Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf)

[Weight Normalization](https://arxiv.org/pdf/1602.07868.pdf)

[Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

[Understanding the Effective Receptive Field in Deep Convolutional Neural Networks](https://arxiv.org/pdf/1701.04128.pdf)

### 07062017

[NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf)

### Print List

[Language Modeling with Gated Convolutional Networks](https://arxiv.org/pdf/1612.08083.pdf)

[Convolutional Sequence to Sequence Learning - Facebook Lastest CNN Seq2Seq](https://arxiv.org/pdf/1705.03122.pdf)

[One Model To Learn Them All](https://arxiv.org/pdf/1706.05137.pdf)

[Attention Is All You Need - Google Lasted CNN Machine Translation](https://arxiv.org/pdf/1706.03762.pdf)

### 07052017

[Language Modeling with Gated Convolutional Networks](https://arxiv.org/pdf/1612.08083.pdf)

[FACTORIZATION TRICKS FOR LSTM NETWORKS](https://arxiv.org/pdf/1703.10722.pdf)

[Neural Machine Translation in Linear Time](https://arxiv.org/pdf/1610.10099.pdf)

[Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/pdf/1606.05328.pdf)


[FREEZEOUT: ACCELERATE TRAINING BY PROGRESSIVELY FREEZING LAYERS](https://arxiv.org/pdf/1706.04983.pdf)

[Language Generation with Recurrent Generative Adversarial Networks without Pre-training](https://arxiv.org/pdf/1706.01399.pdf)

### 07042017

[Convolutional Sequence to Sequence Learning - Facebook Lastest CNN Seq2Seq](https://arxiv.org/pdf/1705.03122.pdf)

[Generating Sequences With Recurrent Neural Networks - Alex Graves RNN](https://arxiv.org/pdf/1308.0850.pdf)

[One Model To Learn Them All](https://arxiv.org/pdf/1706.05137.pdf)

[OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER Hinton](https://arxiv.org/pdf/1701.06538.pdf)

[A CONVOLUTIONAL ENCODER MODEL FOR NEURAL MACHINE TRANSLATION - Facebook CNN Machine Translation](https://arxiv.org/pdf/1611.02344.pdf)

[Attention Is All You Need - Google Lasted CNN Machine Translation](https://arxiv.org/pdf/1706.03762.pdf)

[Network In Network](https://arxiv.org/pdf/1312.4400.pdf)

[MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS](https://arxiv.org/pdf/1511.07122.pdf)

[Depthwise Separable Convolutions for Neural Machine Translation](https://arxiv.org/pdf/1706.03059.pdf)

[Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/pdf/1610.02357.pdf)

### 06232017

[A Convolutional Neural Network for Modelling Sentences](http://www.aclweb.org/anthology/P14-1062)

### 06212017

[One Model To Learn Them All](https://arxiv.org/pdf/1706.05137.pdf)

### 06162017

[Character-Aware Neural Language Models](https://arxiv.org/pdf/1508.06615.pdf)

[TF RNN Tutorial](https://www.tensorflow.org/tutorials/recurrent)

[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[Andrej Karpathy’s The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

[tf rnn unofficial tutorial](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)

[TF RNN Tutorial follows this paper](https://arxiv.org/pdf/1409.2329.pdf)

[Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf)

### 06152017

[Comparison between TF and PyTorch](https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/)

[Deep Reinforcement Learning
from Human Preferences](https://arxiv.org/pdf/1706.03741.pdf)

[CortexNet: a Generic Network Family for
Robust Visual Temporal Representations](https://arxiv.org/pdf/1706.02735.pdf)

[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

[Mask RCNN](https://arxiv.org/pdf/1703.06870.pdf)

[FASTTEXT.ZIP: COMPRESSING TEXT CLASSIFICATION MODELS](https://arxiv.org/pdf/1612.03651.pdf)

[Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)

### 06142017

[Notes on Noise Contrastive Estimation and Negative Sampling](https://arxiv.org/pdf/1410.8251.pdf)

[A Scalable Hierarchical Distributed Language Model](https://pdfs.semanticscholar.org/1005/645c05585c2042e3410daeed638b55e2474d.pdf)

[Learning word embeddings efficiently with
noise-contrastive estimation](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)

[importance sampling](ieeexplore.ieee.org/iel5/72/4479602/04443871.pdf)

### 06132017

[perlexity mit notes](http://web.mit.edu/6.863/www/fall2012/lectures/lecture2&3-notes12.pdf)

[nlp stanford slides](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf)

[skip thought vectors](https://arxiv.org/pdf/1506.06726.pdf)

[pyTorch Tutorial](http://pytorch.org/tutorials/)
### 06122017

[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)

[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)

#### Arxiv Lastest

[Unsupervised object learning from dense equivariant
image labelling](https://arxiv.org/pdf/1706.02932.pdf)

[A Joint Model for Question Answering and Question Generation](https://arxiv.org/pdf/1706.01450.pdf)

[ShiftCNN: Generalized Low-Precision Architecture
for Inference of Convolutional Neural Networks](https://arxiv.org/pdf/1706.02393.pdf)

### 06112017

[The Great A.I. Awakening](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=social&_r=0)

[Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf)

[See, Hear, and Read: Deep Aligned Representations](https://arxiv.org/pdf/1706.00932.pdf)

### 06092017

[Deep Learning: A Bayesian Perspective <this>](https://arxiv.org/pdf/1706.00473.pdf)

[Perplexity](https://en.wikipedia.org/wiki/Perplexity)

### 06082017

[distributed mini-batch](https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf)

[highway network concised](https://arxiv.org/pdf/1505.00387.pdf)

[highway network full paper](https://arxiv.org/pdf/1507.06228.pdf)

[Deep Image: Scaling up Image Recognition](https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf)


### 06072017

Leaky relu

[Res Net read again](https://arxiv.org/pdf/1512.03385.pdf)

[Know about floating numbers](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)

#### Batch-size selections:
[basic hyper-parameters](http://cs231n.github.io/neural-networks-3/)

[Practical Recommendations for Gradient-Based Training of Deep Architectures](https://arxiv.org/pdf/1206.5533.pdf)

[Stochastic Gradient Descent Tricks](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf)

[Efficient BackProb](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

[Efficient Mini-batch Training for Stochastic Optimization](http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf)

[Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/abs/1606.04838)

[On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/pdf/1609.04836.pdf)

https://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent

https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent

https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network




### 06062017

[Very Deep Convolutional Networks for Text Classification](https://arxiv.org/pdf/1606.01781.pdf)

### Long time no read

[optimizing-gradient-descent](http://sebastianruder.com/optimizing-gradient-descent/)

[Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410.pdf)



## History List
[Topics in ML](https://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html)

##This Semester
[Visual Dialog](https://arxiv.org/pdf/1611.08669v2.pdf)

[ORDER-EMBEDDINGS OF IMAGES AND LANGUAGE](https://arxiv.org/pdf/1511.06361v6.pdf)

[Learning Aligned Cross-Modal Representations from Weakly Aligned Data](http://www.cs.toronto.edu/~castrejon/content/cvpr2016.pdf)

[Pixel Recursive Super Resolution](https://arxiv.org/pdf/1702.00783v1.pdf) 

[Tutorial on GAN](https://arxiv.org/pdf/1701.00160v3.pdf)

[NIPS 2016 GAN Papers](https://sites.google.com/site/nips2016adversarial/home/accepted-papers)

[Generating Text via Adversarial Training](https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_20.pdf?attachauth=ANoY7cpDgqvVi1CaTdC4YxVV7h-CfamMefA7wgvpcFFKnycvhSzluG6nOHAjy7Tp1bCIPsruuWBTKaNbZTgnEolWqBGaI7SiFefiS0otYRUM_fu-Fd1lMgLBK6uJHHGPOzTi85LDj4Pj_DpTeGbGTKWDSHbjCMT4XcIiMUONvwicj4wxwf1y1X1greT0T2DmBtmIjh6e1WfFCHKWwBslkh57PqKbD-Z2bnkINNeyJ8Ndj9vEkOPn_FM%3D&attredirects=0)

[Skip-Thought Vectors](https://arxiv.org/pdf/1506.06726.pdf)

[GAN](http://datascienceassn.org/sites/default/files/Generative%20Adversarial%20Nets.pdf)

[Modeling documents with Generative Adversarial Networks](https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_19.pdf?attachauth=ANoY7cpiEhgAdlzMTtlEVkXEJqYZj9yRyLYZ_pw4OyrFE9OhT_qu7TOZPgmGgu5u3GYb6oz_uqVWEF5zGzzVUVadoWD_qaKRys2vovtk8RPtS_b2JqZP2YVbu6SdAwXi_1bn2XUfO2xIYQ-LP4SpR3-Yjq_n1tN9vIm3uGG5A2LXJcSkzMDIvZO7ojMyWkusYwhXZu3zLNem7JazCSoBCfSKGOlw4jU_6vLY5OhI_fm8vOL1r9PefBI%3D&attredirects=0)

[Tony Wu](https://arxiv.org/pdf/1611.04273.pdf)
###Language & RNN

[Distributed Representations of Sentences and Documents](http://cs.stanford.edu/~quocle/paragraph_vector.pdf)

###Recommendations
[NetEase](http://bmc.uestc.edu.cn/~zhangdongxiang/papers/ICDE16_industry_231.pdf)

[Maksims_1](https://pdfs.semanticscholar.org/c52e/f5426715984b1c6440a582499a549d33e4ce.pdf)

[Effective Latent Models for Binary Feedback in Recommender Systems](https://pdfs.semanticscholar.org/49ee/8a342cb3676f334165aa2dd05fab995c00f7.pdf)

[SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS](https://arxiv.org/pdf/1511.06939.pdf)
## Videos
[Ruslan's Tutorial in Deep Learning](https://simons.berkeley.edu/talks/tutorial-deep-learning)

[Stanford NLP Deep Learning syllabus](http://cs224d.stanford.edu/syllabus.html)

[Stanford NLP Deep Learning video list](https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG)

## For Sanja's Project

[VQA overview](https://arxiv.org/pdf/1607.05910.pdf)

[Multi-labeled CNN](https://arxiv.org/pdf/1406.5726.pdf)

### Follow up of VQA Overview
(https://arxiv.org/pdf/1511.02799.pdf)

(https://cs.stanford.edu/people/karpathy/nips2014.pdf)

(https://arxiv.org/pdf/1505.02074.pdf)

(https://arxiv.org/pdf/1506.07285.pdf)

(https://arxiv.org/pdf/1603.01417.pdf)

***(https://arxiv.org/pdf/1511.06973.pdf)***

***(https://arxiv.org/pdf/1505.05612.pdf)***

***(https://arxiv.org/pdf/1606.01847.pdf)***

[UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1511.06434.pdf)

=======================================================================================

(https://arxiv.org/pdf/1511.05960.pdf)

(https://arxiv.org/pdf/1602.04341.pdf)

(https://arxiv.org/pdf/1512.05193.pdf)

(https://arxiv.org/pdf/1502.03044.pdf)

**[Stacked Attention Networks for Image Question Answering](https://arxiv.org/pdf/1511.02274.pdf)

(https://arxiv.org/pdf/1610.01076.pdf)
